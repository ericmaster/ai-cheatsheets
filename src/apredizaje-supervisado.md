# Aprendizaje Supervisado

## üîç Diagn√≥stico Visual: Bias vs Variance

| Escenario                   | Curva de P√©rdida (CV vs Entrenamiento) | Sugerencias                                |
| --------------------------- | -------------------------------------- | ------------------------------------------ |
| **High Bias (Underfit)**    | Ambas curvas altas, peque√±a diferencia | Agregar features, modelo m√°s complejo, disminuir regularizaci√≥n      |
| **High Variance (Overfit)** | Entrenamiento bajo, CV alto            | Regularizar, simplificar modelo, m√°s datos |
| **Buen Fit**                | Ambas bajas, diferencia peque√±a        | Ideal, no cambiar                          |

## üõ†Ô∏è T√©cnicas de Optimizaci√≥n de Hiperpar√°metros

| T√©cnica                         | Aplicaci√≥n                             | Uso recomendado              |
| ------------------------------- | -------------------------------------- | ---------------------------- |
| **K-Fold CV**                   | Divisi√≥n equilibrada                   | Evaluar generalizaci√≥n       |
| **Grid Search / Random Search** | Combinaciones de hiperpar√°metros       | Selecci√≥n de hiperpar√°metros |
| **Nested CV**                   | Selecci√≥n de modelo + evaluaci√≥n real  | Model selection + tuning     |
| **Early stopping**              | Detener entrenamiento antes de overfit | Redes neuronales             |
| **Regularizaci√≥n (L1/L2)**      | Penaliza par√°metros grandes            | Evita overfitting            |

‚ö†Ô∏è No uses el conjunto de test para ajustar hiperpar√°metros; usa validaci√≥n cruzada.

## üßÆ Algoritmos de Clasificaci√≥n/Regresi√≥n y Cu√°ndo Usarlos

| Algoritmo               | Problemas comunes                    | Pros                                            | Contras / Riesgos                                       |
| ----------------------- | ------------------------------------ | ----------------------------------------------- | ------------------------------------------------------- |
| **Regresi√≥n Lineal**    | Predicci√≥n continua, pocos atributos | Simplicidad, interpretabilidad                  | Supone linealidad, sensible a outliers                  |
| **Regresi√≥n Log√≠stica** | Clasificaci√≥n binaria/multiclase     | Probabil√≠stica, r√°pida                          | Inadecuado en relaciones no lineales                    |
| **k-NN**                | Problemas locales con muchos datos   | No param√©trico, simple                          | Costoso computacionalmente, sensible al ruido           |
| **SVM**                 | Texto, imagen, margen definido       | Robusto, kernel trick para no linealidad        | Escalable dif√≠cil, sin probabilidades directas          |
| **√Årboles (DT)**        | Datos tabulares, interpretabilidad   | Interpretables, r√°pido                          | Sensibles a cambios en datos                            |
| **Random Forest**       | Datos tabulares, mezcla compleja     | Reducci√≥n de varianza, robustez                 | Menos interpretables, costoso en memoria                |
| **Boosting (XGBoost)**  | Competencias de precisi√≥n            | Mejores scores en test                          | Overfitting si no se controla regularizaci√≥n            |
| **Redes Neuronales**    | Im√°genes, audio, texto               | Flexible, autoexplotaci√≥n de features complejas | Requieren muchos datos, caja negra, entrenamiento lento |
| **Transfer Learning**   | Im√°genes, NLP con pocos datos        | Uso de modelos preentrenados                    | Dependiente del dominio fuente y fine-tuning cuidadoso  |

## üìä Selecci√≥n de Modelo

```
TIPO DE PROBLEMA ‚îÄ‚ñ∫ ¬øSALIDA?
                    ‚îÇ
                    ‚îú‚îÄ‚ñ∫ Continua (Precio, Edad)
                    ‚îÇ     ‚îú‚îÄ‚ñ∫ Lineal (pocas features) ‚Üí Regresi√≥n Lineal
                    ‚îÇ     ‚îî‚îÄ‚ñ∫ No lineal / compleja ‚Üí kNN, SVR, NN
                    ‚îÇ
                    ‚îî‚îÄ‚ñ∫ Discreta (Clase)
                          ‚îú‚îÄ‚ñ∫ Binaria ‚Üí Regresi√≥n Log√≠stica, SVM
                          ‚îú‚îÄ‚ñ∫ Multiclase ‚Üí √Årboles, SVM, NN
                          ‚îî‚îÄ‚ñ∫ Multietiqueta ‚Üí √Årboles, NN (con salida sigmoid)
```

## üîÑ T√©cnicas de Validaci√≥n Cruzada

| **T√©cnica**               | **Descripci√≥n**                                         | **Uso recomendado**                               |
| ------------------------- | ------------------------------------------------------- | ------------------------------------------------- |
| **Hold-out**              | Divisi√≥n en train/validation/test (ej. 60/20/20)        | Simple, r√°pida, menos robusta                     |
| **K-Fold CV**             | Divide en k subconjuntos; cada uno rota como validaci√≥n | Balance entre sesgo y varianza (t√≠pico: k=5 o 10) |
| **Stratified K-Fold**     | Igual que K-Fold pero conserva proporci√≥n de clases     | Clasificaci√≥n con clases desbalanceadas           |
| **Leave-One-Out (LOOCV)** | Cada muestra se usa una vez como validaci√≥n             | Muy costoso computacionalmente, alta varianza     |
| **Nested CV**             | CV interna para tuning y externa para evaluaci√≥n        | Evita optimismo al ajustar hiperpar√°metros        |

## üìä M√©tricas de Evaluaci√≥n

### üîπ Clasificaci√≥n

| **M√©trica** | **F√≥rmula / Significado**                                   | **Uso recomendado**                  |
| ----------- | ----------------------------------------------------------- | ------------------------------------ |
| Accuracy    | $\frac{TP + TN}{TP + TN + FP + FN}$                         | Datasets balanceados                 |
| Precision   | $\frac{TP}{TP + FP}$                                        | Alta penalizaci√≥n a falsos positivos |
| Recall      | $\frac{TP}{TP + FN}$                                        | Alta penalizaci√≥n a falsos negativos |
| F1-Score    | $2 \cdot \frac{precision \cdot recall}{precision + recall}$ | Compromiso entre precision y recall  |
| ROC-AUC     | √Årea bajo curva ROC (TPR vs FPR)                            | Comparaci√≥n global de clasificadores |
| PR-AUC      | √Årea bajo curva Precisi√≥n vs Recall                         | M√°s √∫til en datasets desbalanceados  |


### üîπ Regresi√≥n

| **M√©trica** | **F√≥rmula / Significado**              | **Uso recomendado**                           |   |                           |
| ----------- | -------------------------------------- | --------------------------------------------- | - | ------------------------- |
| MSE         | $\frac{1}{n} \sum (y_i - \hat{y}_i)^2$ | Penaliza errores grandes, sensible a outliers |   |                           |
| RMSE        | $\sqrt{MSE}$                           | Interpretable en unidades originales          |   |                           |
| MAE         | $\frac{1}{n} \sum \|y_i - \hat{y}_i\|$ | M√°s robusto ante outliers |
| $R^2$ Score | $1 - \frac{SS_{res}}{SS_{tot}}$        | Proporci√≥n de varianza explicada              |   |                           |

### üß≠ Gu√≠a para Toma de Decisiones

| **Escenario**                            | **Decisi√≥n recomendada**                                  |
| ---------------------------------------- | --------------------------------------------------------- |
| Datos balanceados, clasificaci√≥n binaria | Usa **accuracy** y **F1** si importa la precisi√≥n general |
| Datos desbalanceados                     | Usa **Precision/Recall**, **F1**, y **PR-AUC**            |
| Regresi√≥n con outliers                   | Usa **MAE**                                               |
| Optimizaci√≥n de hiperpar√°metros          | Usa **K-Fold CV** o **Nested CV**                         |
| Comparar modelos con m√©tricas similares  | Usa test estad√≠stico o curvas ROC/PR                      |
| Detectar overfitting                     | Comparar m√©tricas entre training y validation             |


## üìã Regularizaci√≥n

| **Aspecto**                    | **L1 (Lasso)**                                                                         | **L2 (Ridge)**                                           | **Uso Com√∫n / Consejos**                                                                |                           |                                                                |
| ------------------------------ | -------------------------------------------------------------------------------------- | -------------------------------------------------------- | --------------------------------------------------------------------------------------- | ------------------------- | -------------------------------------------------------------- |
| **Definici√≥n**                 | Penaliza la suma de valores absolutos de los coeficientes                              | Penaliza la suma de los cuadrados de los coeficientes    | Ambas t√©cnicas reducen la complejidad del modelo para evitar overfitting                |                           |                                                                |
| **Efecto en los coeficientes** | Tiende a hacer que algunos coeficientes sean exactamente cero (selecci√≥n de variables) | Reduce los coeficientes sin hacerlos cero                | L1 √∫til si se busca **sparse model**, L2 √∫til cuando todas las variables aportan        |                           |                                                                |
| **Diagn√≥stico aplicado**       | Alto **variance** ‚Üí aplicar L1 o L2                                                    | Alto **variance** ‚Üí aplicar L2                           | En casos con high bias, usar **menos regularizaci√≥n**                                   |                           |                                                                |
| **Tipo de problema**           | Modelos interpretables con muchas features                                             | Modelos con colinealidad                                 | L1 mejor para selecci√≥n autom√°tica de variables, L2 mejor para evitar multicolinealidad |                           |                                                                |
| **Modelos aplicables**         | Regresi√≥n lineal/log√≠stica, SVM, NN (con dropout)                                      | Igual que L1                                             | Regularizaci√≥n puede usarse en la mayor√≠a de los modelos param√©tricos                   |                           |                                                                |
| **Riesgos si se aplica mal**   | Demasiado grande Œª puede causar **underfitting**                                       | Igual: penalizaci√≥n excesiva reduce capacidad del modelo | Usar **curvas de validaci√≥n** para ajustar Œª                                            |                           |                                                                |

### üß≠ Sugerencias Pr√°cticas para Toma de Decisiones
- ¬øDemasiado buen rendimiento en entrenamiento pero malo en validaci√≥n?
‚Üí Prueba regularizaci√≥n (aumentar Œª)

- ¬øModelo muy complejo o lento?
‚Üí Prueba L1 para seleccionar features relevantes

- ¬øDatos altamente correlacionados?
‚Üí Usa L2 (Ridge) para estabilizar los coeficientes

- ¬øProblemas con overfitting en redes neuronales?
‚Üí Combina L2 con early stopping o usa dropout

## üìã Redes Neuronales (NN)

| **Aspecto**                    | **Descripci√≥n / Detalles**                                                                         |
| ------------------------------ | -------------------------------------------------------------------------------------------------- |
| **Objetivo principal**         | Modelar relaciones complejas y no lineales entre entradas y salidas                                |
| **Arquitectura b√°sica**        | Capas: entrada ‚Üí ocultas (hidden layers) ‚Üí salida<br>Neuronas con funciones de activaci√≥n          |
| **Funci√≥n de activaci√≥n**      | ReLU (default en capas ocultas), Sigmoid/Softmax (salidas), Tanh, Leaky ReLU                       |
| **Cost function**              | - Regresi√≥n: MSE<br>- Clasificaci√≥n binaria: BCE<br>- Multiclase: Categorical Cross-Entropy        |
| **Capacidades clave**          | - Aprende representaciones jer√°rquicas<br>- Adecuado para datos no estructurados (im√°genes, texto) |
| **T√©cnicas de optimizaci√≥n**   | Descenso por gradiente con variantes: SGD, Adam, RMSProp                                           |
| **Ventajas**                   | - Flexible y poderosa para datos grandes y complejos<br>- Automatiza extracci√≥n de features        |
| **Desventajas**                | - Requiere muchos datos<br>- Tiempo de entrenamiento alto<br>- Dif√≠cil interpretabilidad           |
| **¬øCu√°ndo usar NN?**           | - Visi√≥n por computador, NLP, secuencias, relaciones no lineales                                   |
| **¬øCu√°ndo evitar NN?**         | - Pocos datos<br>- Necesidad alta de interpretabilidad<br>- Modelos tabulares simples              |
| **T√©cnicas de regularizaci√≥n** | Dropout, L2 (weight decay), batch normalization, early stopping                                    |
| **Diagn√≥stico com√∫n**          | - High bias ‚Üí m√°s neuronas/capas, cambiar activaci√≥n<br>- High variance ‚Üí regularizaci√≥n           |
| **Frameworks populares**       | TensorFlow, Keras, PyTorch                                                                         |

### üß≠ Sugerencias para la Toma de Decisiones

| **Situaci√≥n**                                        | **Recomendaci√≥n**                                                |
| ---------------------------------------------------- | ---------------------------------------------------------------- |
| Muchas features no lineales o datos no estructurados | Usa NN: puede aprender patrones complejos (imagen, texto, audio) |
| Alta precisi√≥n en clasificaci√≥n con pocos datos      | Mejor usar modelos como SVM, Random Forest                       |
| Entrenamiento lento o sin GPU                        | Considera modelos m√°s simples (Regresi√≥n, √Årboles)               |
| Modelo con sobreajuste                               | Aplica dropout, L2, o early stopping                             |
| Predicci√≥n secuencial o multietiqueta compleja       | Usa arquitecturas espec√≠ficas: RNN, LSTM, Transformers           |

## üìã Transfer Learning

| **Aspecto**                | **Descripci√≥n / Detalles**                                                                                   |
| -------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Definici√≥n**             | T√©cnica que reutiliza un modelo preentrenado en una tarea diferente pero relacionada                         |
| **Objetivo**               | Aprovechar conocimiento existente para evitar entrenar modelos desde cero                                    |
| **Arquitecturas comunes**  | Imagen: VGG, ResNet, MobileNet, Inception, YOLO<br>Texto: BERT, GPT, RoBERTa                                 |
| **Tipos de transferencia** | - **Feature Extraction**: usar pesos congelados<br>- **Fine-Tuning**: reentrenar capas                       |
| **¬øCu√°ndo usar?**          | - Datos limitados<br>- Tarea similar a dataset grande existente (ej. ImageNet, BERT)                         |
| **¬øCu√°ndo evitar?**        | - Tarea muy diferente a la del modelo fuente<br>- Dominio altamente especializado                            |
| **Beneficios clave**       | - Ahorro computacional<br>- Menor necesidad de datos<br>- Mejores resultados iniciales                       |
| **Requerimientos**         | - Modelo base preentrenado<br>- Conjunto de datos etiquetados para tarea destino                             |
| **Desventajas**            | - Riesgo de **negative transfer** si dominios no coinciden<br>- Sensible a overfitting si no se congela bien |
| **Decisi√≥n entre m√©todos** | - **Pocos datos** ‚Üí feature extraction<br>- **Datos medianos y similares** ‚Üí fine-tuning                     |
| **Criterios para ajuste**  | - Reducir learning rate en fine-tuning<br>- Evitar sobreajuste al descongelar                                |
| **Usos populares**         | Clasificaci√≥n de im√°genes, an√°lisis de sentimientos, detecci√≥n de objetos, clasificaci√≥n m√©dica              |

### üß≠ Sugerencias para la Toma de Decisiones

| **Escenario**                                                 | **Decisi√≥n recomendada**                                    |
| ------------------------------------------------------------- | ----------------------------------------------------------- |
| Tienes pocos datos pero tarea similar (e.g. imagenes m√©dicas) | Usa modelo preentrenado con **feature extraction**          |
| Datos moderados y dominio similar                             | Usa **fine-tuning**: descongela capas superiores            |
| Tu tarea es muy distinta (imagen ‚Üí audio)                     | Transfer learning **no recomendado**                        |
| Problemas de sobreajuste en fine-tuning                       | Disminuir learning rate, congelar capas iniciales           |
| Requiere entrenamiento r√°pido y eficaz                        | Transfer learning con capas personalizadas y base congelada |

## üìã k-Nearest Neighbors (k-NN)

| **Aspecto**                    | **Descripci√≥n / Detalles**                                                                                        |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------------- |
| **Definici√≥n**                 | Algoritmo basado en memoria: clasifica o estima seg√∫n los k puntos m√°s cercanos                                   |
| **Tipo de modelo**             | No param√©trico, aprendizaje "perezoso" (lazy learning), sin entrenamiento expl√≠cito                               |
| **Funci√≥n de decisi√≥n**        | - Clasificaci√≥n: mayor√≠a entre vecinos<br>- Regresi√≥n: promedio de los valores de vecinos                         |
| **M√©tricas de distancia**      | Euclidiana, Manhattan, Coseno, Pearson ‚Äî seg√∫n el dominio y escala                                                |
| **Preprocesamiento necesario** | Normalizaci√≥n/estandarizaci√≥n de caracter√≠sticas (para distancias significativas)                                 |
| **¬øCu√°ndo usar?**              | - Dataset peque√±o a mediano<br>- Relaciones locales fuertes<br>- Pocas dimensiones                                |
| **¬øCu√°ndo evitar?**            | - Muchos atributos irrelevantes<br>- Alta dimensionalidad (curse of dimensionality)<br>- Datos escasos o ruidosos |
| **Ventajas**                   | Simple, sin suposiciones sobre la distribuci√≥n, adaptable a problemas multiclase                                  |
| **Desventajas**                | Costoso en predicci√≥n, sensible a ruido y escalamiento, requiere almacenamiento completo                          |
| **Par√°metro clave (k)**        | - k peque√±o: alta varianza, m√°s sensible<br>- k grande: m√°s estable, pero puede suavizar demasiado                |
| **Pesado vs. No pesado**       | Ponderaci√≥n por distancia mejora rendimiento en datasets con densidad variable                                    |
| **Uso en regresi√≥n**           | Promedio de valores de los k vecinos m√°s cercanos                                                                 |
| **Uso en clasificaci√≥n**       | Voto de mayor√≠a entre clases de los k vecinos                                                                     |
### üß≠ Sugerencias para la Toma de Decisiones

| **Escenario**                            | **Decisi√≥n recomendada**                                         |
| ---------------------------------------- | ---------------------------------------------------------------- |
| Pocos datos y relaciones locales simples | Usa **k-NN con k=3 o 5**                                         |
| Muchas features no escaladas             | Aplica **escalado de datos** antes de usar k-NN                  |
| Datos ruidosos o outliers                | Usa **k m√°s grande o versi√≥n ponderada**                         |
| Predicci√≥n lenta o muchos datos          | Considera modelos m√°s r√°pidos: SVM, √°rboles, regresi√≥n log√≠stica |
| Altas dimensiones (>20-30 features)      | Aplica **PCA** o selecci√≥n de caracter√≠sticas                    |

## üìã Support Vector Machines (SVM)

| **Aspecto**                     | **Descripci√≥n / Detalles**                                                                                     |
| ------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| **Objetivo**                    | Encontrar el hiperplano √≥ptimo que **maximiza el margen** entre clases                                         |
| **Tipo de modelo**              | Modelo discriminativo, **margin-based**, eficaz para clasificaci√≥n lineal y no lineal                          |
| **Casos de uso t√≠picos**        | Texto, im√°genes, bioinform√°tica, clasificaci√≥n binaria                                                         |
| **¬øLineal o no lineal?**        | Lineal: cuando los datos son separables<br>No lineal: usa el **truco del kernel**                              |
| **Kernel Trick**                | Computa productos internos en espacio de alta dimensi√≥n sin mapear expl√≠citamente                              |
| **Kernels comunes**             | - Lineal<br>- Polinomial<br>- RBF (Gaussian)<br>- Sigmoide                                                     |
| **Hiperpar√°metros clave**       | - **C**: penaliza errores (soft margin)<br>- **Œ≥**: ancho de RBF (si se usa RBF)                               |
| **¬øCu√°ndo usar?**               | - Dataset con pocas features<br>- Margen claro entre clases<br>- Datos no muy grandes                          |
| **¬øCu√°ndo evitar?**             | - Datos ruidosos con muchos outliers<br>- Problemas multiclase complejos<br>- Escalabilidad                    |
| **Ventajas**                    | - Precisi√≥n alta<br>- Efectivo en espacios de alta dimensi√≥n<br>- Robusto al overfitting                       |
| **Desventajas**                 | - Costoso en entrenamiento<br>- No produce probabilidades directamente<br>- Dif√≠cil de ajustar para multiclase |
| **SVM vs. Regresi√≥n Log√≠stica** | SVM se enfoca en el margen geom√©trico, RL en la probabilidad y entrop√≠a cruzada                                |
| **Aplicaci√≥n en regresi√≥n**     | **SVR**: minimiza un margen tolerante alrededor del valor real                                                 |

### üß≠ Sugerencias para la Toma de Decisiones

| **Situaci√≥n**                                  | **Recomendaci√≥n**                                                      |
| ---------------------------------------------- | ---------------------------------------------------------------------- |
| Pocos datos y gran separaci√≥n entre clases     | Usa **SVM con kernel lineal**                                          |
| Clasificaci√≥n no lineal con patrones complejos | Usa **kernel RBF o polinomial**                                        |
| Hay muchos outliers en los datos               | Ajusta el par√°metro **C (menor valor)** para permitir margen suave     |
| Dataset muy grande (>10,000 muestras)          | Considera modelos m√°s escalables: √°rboles, regresi√≥n log√≠stica         |
| Necesitas probabilidades                       | Usa regresi√≥n log√≠stica o calibraci√≥n externa (Platt scaling para SVM) |

## üìã Decision Trees

| **Aspecto**                   | **Descripci√≥n / Detalles**                                                                     |
| ----------------------------- | ---------------------------------------------------------------------------------------------- |
| **Objetivo principal**        | Dividir el espacio de decisiones en regiones homog√©neas mediante condiciones l√≥gicas           |
| **Tipo de modelo**            | Modelo no param√©trico, interpretable, basado en reglas if-then                                 |
| **Aplicaciones comunes**      | Clasificaci√≥n, regresi√≥n, segmentaci√≥n, an√°lisis exploratorio                                  |
| **Criterios de divisi√≥n**     | - Clasificaci√≥n: **Entrop√≠a**, **√çndice Gini**<br>- Regresi√≥n: **Varianza / MSE**              |
| **Componentes del √°rbol**     | - Nodo ra√≠z, nodos internos (condiciones), hojas (predicciones)                                |
| **Ventajas**                  | - Muy interpretable<br>- No requiere escalado de features<br>- Maneja datos categ√≥ricos        |
| **Desventajas**               | - Sensible a peque√±as variaciones en los datos<br>- Propenso a **overfitting**                 |
| **¬øCu√°ndo usar?**             | - Reglas de decisi√≥n claras<br>- Datos tabulares con relaciones no lineales                    |
| **¬øCu√°ndo evitar?**           | - Datos ruidosos<br>- Requiere alta generalizaci√≥n sin ensemble                                |
| **Regularizaci√≥n**            | - Profundidad m√°xima (max\_depth)<br>- N√∫mero m√≠nimo de muestras por nodo (min\_samples)       |
| **Detenci√≥n del crecimiento** | - Cuando se alcanza pureza total<br>- Cuando no hay ganancia significativa de informaci√≥n      |
| **Mejoras comunes**           | Usar **pruning** (poda), **early stopping**, o emplear ensembles como Random Forest o Boosting |

### üß≠ Sugerencias para la Toma de Decisiones

| **Situaci√≥n**                                     | **Decisi√≥n recomendada**                                                |
| ------------------------------------------------- | ----------------------------------------------------------------------- |
| Se requiere un modelo **altamente interpretable** | Usa Decision Tree con poda y profundidad limitada                       |
| Modelo sobreajusta mucho                          | Ajusta **max\_depth**, **min\_samples\_split**, o usa **Random Forest** |
| Problema con muchas features irrelevantes         | Considera selecci√≥n de caracter√≠sticas o usar √°rbol con regularizaci√≥n  |
| ¬øM√©tricas similares en divisiones posibles?       | Prefiere la que maximice **ganancia de informaci√≥n**                    |
| Clasificaci√≥n binaria con clases balanceadas      | Usa **√≠ndice Gini** o **entrop√≠a**; ambos funcionan bien                |
