| **Tema**                | **Fórmula**                                                                                   | **Descripción / Uso**                                        |        |                                           |
| ----------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------ | ------ | ----------------------------------------- |
| **Regresión Lineal**    | $h_\theta(x) = \theta_0 + \theta_1 x + \dots + \theta_n x_n$                                  | Hipótesis lineal para predicción de salida continua          |        |                                           |
|                         | $J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2$                       | Función de costo (MSE) en regresión lineal                   |        |                                           |
| **Regresión Logística** | $h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}$                                                 | Función sigmoide que produce probabilidad de clase           |        |                                           |
|                         | $J(\theta) = -\frac{1}{m} \sum [y \log(h_\theta) + (1 - y) \log(1 - h_\theta)]$               | Costo log-loss o binary cross-entropy                        |        |                                           |
| **Regularización**      | $J(\theta) + \lambda \sum_{j=1}^n \theta_j^2$ (L2)                                            | Penalización para controlar sobreajuste (Ridge)              |        |                                           |
|                         | $J(\theta) + \lambda \sum_{j=1}^n  \| \theta_j\|$ (L1) | Penalización que induce sparsidad (Lasso) |
| **PCA**                 | $Z = U^T X$                                                                                   | Proyección de datos sobre componentes principales            |        |                                           |
|                         | $\Sigma = \frac{1}{m} X^T X$                                                                  | Matriz de covarianza para PCA                                |        |                                           |
| **SVM**                 | $\min \frac{1}{2} \|w\|^2 + C \sum \xi_i$                                                     | Objetivo de margen suave con penalización C                  |        |                                           |
|                         | $K(x, x') = \exp(-\gamma \|x - x'\|^2)$                                                       | Kernel RBF (gaussiano)                                       |        |                                           |
| **k-NN**                | $\hat{y}_q = \frac{1}{k} \sum_{j=1}^k y_{NN_j}$                                               | Promedio de vecinos para regresión                           |        |                                           |
|                         | $\text{dist}(x, x') = \sqrt{\sum_{i=1}^n (x_i - x'_i)^2}$                                     | Distancia euclidiana                                         |        |                                           |
| **Redes Neuronales**    | $a^{(l)} = g(W^{(l)} a^{(l-1)} + b^{(l)})$                                                    | Propagación hacia adelante en capa l                         |        |                                           |
|                         | $\sigma(z) = \frac{1}{1 + e^{-z}}$                                                            | Activación sigmoide                                          |        |                                           |
|                         | $\text{ReLU}(z) = \max(0, z)$                                                                 | Activación ReLU                                              |        |                                           |
|                         | $\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$                                        | Probabilidades multiclase                                    |        |                                           |
| **Evaluación modelos**  | $\text{Accuracy} = \frac{\text{\# aciertos}}{\text{total de ejemplos}}$                       | Métrica de exactitud                                         |        |                                           |
|                         | $\text{Precision} = \frac{TP}{TP + FP}$                                                       | Proporción de verdaderos positivos entre predichos positivos |        |                                           |
|                         | $\text{Recall} = \frac{TP}{TP + FN}$                                                          | Proporción de positivos reales bien clasificados             |        |                                           |
|                         | $F_1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}$ | Media armónica entre precisión y recall                      |        |                                           |
|                         | $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$                                                         | Coeficiente de determinación en regresión                    |        |                                           |
